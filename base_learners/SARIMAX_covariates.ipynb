{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import holidays\n",
    "from prophet import Prophet\n",
    "import pickle\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "load = pd.read_csv('data/load.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df = pd.read_csv('data/load.csv')\n",
    "hierarchy_df = pd.read_csv(\"data/hierarchy.csv\")\n",
    "humidity_df = pd.read_csv(\"data/relative humidity.csv\")\n",
    "temperature_df = pd.read_csv(\"data/temperature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_formatting(load_df, hierarchy_df, humidity_df, temperature_df):\n",
    "    load_df = pd.melt(load_df, id_vars=[\"meter_id\", \"date\"], value_vars=load_df.columns.difference([\"meter_id\", \"date\"]),\n",
    "                                var_name=\"hour\", value_name=\"load\")\n",
    "    load_df[\"hour\"] = load_df[\"hour\"].str.strip(\"h\").astype(int) - 1\n",
    "    load_df[\"timestamp\"] = pd.to_datetime(load_df[\"date\"] + \" \" + load_df[\"hour\"].astype(str) + \":00:00\", format=\"%m/%d/%Y %H:%M:%S\")\n",
    "    load_df[\"meter_id\"] = load_df[\"meter_id\"].astype(int)\n",
    "    load_df = load_df.drop(columns=[\"date\", \"hour\"])\n",
    "    # Remove meter ids that appear in training but not in test and conversely\n",
    "    aggregate_list = [('max_timestamp', 'max'), ('min_timestamp', 'min')]\n",
    "    meters_df = load_df.groupby(\"meter_id\")[\"timestamp\"].agg(aggregate_list).reset_index()\n",
    "    excluded_meters_df = meters_df[(meters_df[\"max_timestamp\"]<dt.datetime(2011,1,1)) | (meters_df[\"min_timestamp\"]>dt.datetime(2011,1,1))]\n",
    "    excluded_meters = excluded_meters_df[\"meter_id\"].to_list() + [236]\n",
    "    load_df = load_df[~load_df[\"meter_id\"].isin(excluded_meters)]\n",
    "    # Ensure that all training points are present, being possibly 0\n",
    "    date_rng = pd.date_range(start='2005-01-01', end='2010-12-31 23:00:00', freq='1H')\n",
    "    all_combinations = pd.MultiIndex.from_product([load_df['meter_id'].unique(), date_rng], names=['meter_id', 'timestamp'])\n",
    "    df_all_combinations = pd.DataFrame(index=all_combinations).reset_index()\n",
    "    df_filled = pd.merge(df_all_combinations, load_df, on=['meter_id', 'timestamp'], how='left', suffixes=('_orig', '')).fillna(0)\n",
    "    df_filled = pd.concat([df_filled, load_df[load_df[\"timestamp\"]>=dt.datetime(2011,1,1)]])\n",
    "    df_filled = df_filled.sort_values(by=[\"meter_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    data_df = df_filled.merge(hierarchy_df, on=\"meter_id\", how=\"left\")\n",
    "    data_df[hierarchy_df.columns.difference([\"meter_id\"])] = data_df[hierarchy_df.columns.difference([\"meter_id\"])].astype(str)\n",
    "\n",
    "    humidity_df[\"timestamp\"] = pd.to_datetime(humidity_df[\"date\"] + \" \" + (humidity_df[\"hr\"] - 1).astype(str) + \":00:00\", format=\"%d%b%Y %H:%M:%S\")\n",
    "    temperature_df[\"timestamp\"] = pd.to_datetime(temperature_df[\"date\"] + \" \" + (temperature_df[\"hr\"] - 1).astype(str) + \":00:00\", format=\"%d%b%Y %H:%M:%S\")\n",
    "    humidity_df = humidity_df.drop(columns=[\"date\", \"hr\"])\n",
    "    temperature_df = temperature_df.drop(columns=[\"date\", \"hr\"])\n",
    "    temperature_df[\"avg_temperature\"] = temperature_df.filter(like=\"t_\").mean(axis=1)\n",
    "    humidity_df[\"avg_humidity\"] = humidity_df.filter(like=\"rh_\").mean(axis=1)\n",
    "    temperature_df = temperature_df[[\"timestamp\", \"avg_temperature\"]]\n",
    "    humidity_df = humidity_df[[\"timestamp\", \"avg_humidity\"]]\n",
    "    data_df = data_df.merge(humidity_df, on=\"timestamp\", how=\"left\")\n",
    "    data_df = data_df.merge(temperature_df, on=\"timestamp\", how=\"left\")\n",
    "    return data_df\n",
    "\n",
    "joined_data_df = df_formatting(load_df, hierarchy_df, humidity_df, temperature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df):\n",
    "  aggregation_dict = {\"load\": \"sum\"}\n",
    "  aggregation_dict.update({col: \"first\" for col in df.columns if col not in [\"load\", \"timestamp\"]})\n",
    "  top_level_df = df.groupby(\"timestamp\").agg(aggregation_dict).reset_index()\n",
    "  aggregate_level_names = pd.unique(df[\"aggregate\"])\n",
    "  aggregate_levels = {}\n",
    "  for agg_level in aggregate_level_names:\n",
    "    agg_level_df = df[df[\"aggregate\"] == agg_level].copy()\n",
    "    agg_level_df = agg_level_df.groupby(\"timestamp\").agg(aggregation_dict).reset_index()\n",
    "    aggregate_levels[agg_level] = agg_level_df\n",
    "  mid_level_names = pd.unique(df[\"mid_level\"])\n",
    "  mid_levels = {}\n",
    "  for mid_level in mid_level_names:\n",
    "    mid_level_df = df[df[\"mid_level\"] == mid_level].copy()\n",
    "    mid_level_df = mid_level_df.copy().groupby(\"timestamp\").agg(aggregation_dict).reset_index()\n",
    "    mid_levels[mid_level] = mid_level_df\n",
    "  bottom_level_names = pd.unique(df[\"meter_id\"])\n",
    "  bottom_levels = {}\n",
    "  for bottom_level in bottom_level_names:\n",
    "    bottom_level_df = df[df[\"meter_id\"] == bottom_level].copy()\n",
    "    bottom_level_df = bottom_level_df.copy().groupby(\"timestamp\").agg(aggregation_dict).reset_index()\n",
    "    bottom_levels[bottom_level] = bottom_level_df\n",
    "  return top_level_df, aggregate_levels, mid_levels, bottom_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_df, aggregate_levels, mid_levels, bottom_levels = get_datasets(joined_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, encode_meter_id):\n",
    "    # Basic feature engineering: indicator of day of the week, month, is_holiday (in MA), one-hot encoding of hierarchies and of the meter id\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    df['cos_month'] = np.cos(2*np.pi*(df['timestamp'].dt.month/12))\n",
    "    df['sin_month'] = np.sin(2*np.pi*(df['timestamp'].dt.month/12))\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    holidays_MA = holidays.US(years=range(2005, 2012), state=\"MA\")\n",
    "    df['is_holiday'] = df['timestamp'].dt.date.isin(holidays_MA.keys())\n",
    "    df['cos_hour'] = np.cos(2*np.pi*(df['timestamp'].dt.hour/24))\n",
    "    df['sin_hour'] = np.sin(2*np.pi*(df['timestamp'].dt.hour/24))\n",
    "    #df['holiday_name'] = df.apply(lambda row: \"None\" if not df['is_holiday'] else holidays_MA[row[\"timestamp\"].date()], axis=1): too slow, to be optimized\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    onehots = []\n",
    "    if encode_meter_id:\n",
    "      meter_onehot = pd.get_dummies(df['meter_id'], prefix=\"meter\")\n",
    "      onehots.append(meter_onehot)\n",
    "    df = df.drop(columns=[\"meter_id\", \"mid_level\", \"aggregate\"])\n",
    "    dow_onehot = pd.get_dummies(df['day_of_week'], prefix=\"dow\")\n",
    "    df = df.drop(columns=[\"day_of_week\"])\n",
    "    features_df = pd.concat([df, dow_onehot] + onehots, axis=1)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_df = feature_engineering(top_level_df, False)\n",
    "for agg in aggregate_levels.keys():\n",
    "  aggregate_levels[agg] = feature_engineering(aggregate_levels[agg], False)\n",
    "for mid_level in mid_levels.keys():\n",
    "  mid_levels[mid_level] = feature_engineering(mid_levels[mid_level], False)\n",
    "for bottom_level in bottom_levels.keys():\n",
    "  bottom_levels[bottom_level] = feature_engineering(bottom_levels[bottom_level], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    train_df = df[df[\"year\"] < 2011].fillna(0)\n",
    "    test_df = df[df[\"year\"] == 2011].dropna()\n",
    "    return train_df, test_df\n",
    "top_level_train, top_level_test = train_test_split(top_level_df)\n",
    "aggregate_train = {}\n",
    "aggregate_test = {}\n",
    "mid_levels_train = {}\n",
    "mid_levels_test = {}\n",
    "bottom_levels_train = {}\n",
    "bottom_levels_test = {}\n",
    "for agg in aggregate_levels.keys():\n",
    "  train_df, test_df = train_test_split(aggregate_levels[agg])\n",
    "  aggregate_train[agg] = train_df\n",
    "  aggregate_test[agg] = test_df\n",
    "for mid_level in mid_levels.keys():\n",
    "  train_df, test_df = train_test_split(mid_levels[mid_level])\n",
    "  mid_levels_train[mid_level] = train_df\n",
    "  mid_levels_test[mid_level] = test_df\n",
    "for bottom_level in bottom_levels.keys():\n",
    "  train_df, test_df = train_test_split(bottom_levels[bottom_level])\n",
    "  bottom_levels_train[bottom_level] = train_df\n",
    "  bottom_levels_test[bottom_level] = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA with covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_train.to_csv(\"data/temp/top_level_train.csv\", index=False)\n",
    "top_level_test.to_csv(\"data/temp/top_level_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\louis\\OneDrive\\Documents\\Scolaire\\Stanford\\CS 229B\\Project\\CS229B_final_project\\SARIMAX_covariates.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m exog \u001b[39m=\u001b[39m top_level_train\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcos_hour\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msin_hour\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdow_0\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m top_model \u001b[39m=\u001b[39m SARIMAX(endog, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             order\u001b[39m=\u001b[39m(p, \u001b[39m1\u001b[39m, q),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                             seasonal_order\u001b[39m=\u001b[39m(P, \u001b[39m1\u001b[39m, Q, \u001b[39m24\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             freq \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             exog \u001b[39m=\u001b[39m exog)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m top_model \u001b[39m=\u001b[39m top_model\u001b[39m.\u001b[39;49mfit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/louis/OneDrive/Documents/Scolaire/Stanford/CS%20229B/Project/CS229B_final_project/SARIMAX_covariates.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m top_model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodels/SARIMAX/top_model.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:704\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         flags[\u001b[39m'\u001b[39m\u001b[39mhessian_method\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m optim_hessian\n\u001b[0;32m    703\u001b[0m     fargs \u001b[39m=\u001b[39m (flags,)\n\u001b[1;32m--> 704\u001b[0m     mlefit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(MLEModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfit(start_params, method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    705\u001b[0m                                        fargs\u001b[39m=\u001b[39mfargs,\n\u001b[0;32m    706\u001b[0m                                        maxiter\u001b[39m=\u001b[39mmaxiter,\n\u001b[0;32m    707\u001b[0m                                        full_output\u001b[39m=\u001b[39mfull_output,\n\u001b[0;32m    708\u001b[0m                                        disp\u001b[39m=\u001b[39mdisp, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    709\u001b[0m                                        skip_hessian\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    711\u001b[0m \u001b[39m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[39mif\u001b[39;00m return_params:\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py:563\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_t\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    562\u001b[0m optimizer \u001b[39m=\u001b[39m Optimizer()\n\u001b[1;32m--> 563\u001b[0m xopt, retvals, optim_settings \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49m_fit(f, score, start_params,\n\u001b[0;32m    564\u001b[0m                                                fargs, kwargs,\n\u001b[0;32m    565\u001b[0m                                                hessian\u001b[39m=\u001b[39;49mhess,\n\u001b[0;32m    566\u001b[0m                                                method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m    567\u001b[0m                                                disp\u001b[39m=\u001b[39;49mdisp,\n\u001b[0;32m    568\u001b[0m                                                maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[0;32m    569\u001b[0m                                                callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    570\u001b[0m                                                retall\u001b[39m=\u001b[39;49mretall,\n\u001b[0;32m    571\u001b[0m                                                full_output\u001b[39m=\u001b[39;49mfull_output)\n\u001b[0;32m    572\u001b[0m \u001b[39m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    573\u001b[0m optim_settings\u001b[39m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py:241\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    238\u001b[0m     fit_funcs\u001b[39m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    240\u001b[0m func \u001b[39m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 241\u001b[0m xopt, retvals \u001b[39m=\u001b[39m func(objective, gradient, start_params, fargs, kwargs,\n\u001b[0;32m    242\u001b[0m                      disp\u001b[39m=\u001b[39;49mdisp, maxiter\u001b[39m=\u001b[39;49mmaxiter, callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    243\u001b[0m                      retall\u001b[39m=\u001b[39;49mretall, full_output\u001b[39m=\u001b[39;49mfull_output,\n\u001b[0;32m    244\u001b[0m                      hess\u001b[39m=\u001b[39;49mhessian)\n\u001b[0;32m    246\u001b[0m optim_settings \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m: method, \u001b[39m'\u001b[39m\u001b[39mstart_params\u001b[39m\u001b[39m'\u001b[39m: start_params,\n\u001b[0;32m    247\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: maxiter, \u001b[39m'\u001b[39m\u001b[39mfull_output\u001b[39m\u001b[39m'\u001b[39m: full_output,\n\u001b[0;32m    248\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp, \u001b[39m'\u001b[39m\u001b[39mfargs\u001b[39m\u001b[39m'\u001b[39m: fargs, \u001b[39m'\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m'\u001b[39m: callback,\n\u001b[0;32m    249\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mretall\u001b[39m\u001b[39m'\u001b[39m: retall, \u001b[39m\"\u001b[39m\u001b[39mextra_fit_funcs\u001b[39m\u001b[39m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    250\u001b[0m optim_settings\u001b[39m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\optimizer.py:651\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39melif\u001b[39;00m approx_grad:\n\u001b[0;32m    649\u001b[0m     func \u001b[39m=\u001b[39m f\n\u001b[1;32m--> 651\u001b[0m retvals \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39mfmin_l_bfgs_b(func, start_params, maxiter\u001b[39m=\u001b[39mmaxiter,\n\u001b[0;32m    652\u001b[0m                                  callback\u001b[39m=\u001b[39mcallback, args\u001b[39m=\u001b[39mfargs,\n\u001b[0;32m    653\u001b[0m                                  bounds\u001b[39m=\u001b[39mbounds, disp\u001b[39m=\u001b[39mdisp,\n\u001b[0;32m    654\u001b[0m                                  \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kwargs)\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m full_output:\n\u001b[0;32m    657\u001b[0m     xopt, fopt, d \u001b[39m=\u001b[39m retvals\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:197\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39m# build options\u001b[39;00m\n\u001b[0;32m    186\u001b[0m opts \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp,\n\u001b[0;32m    187\u001b[0m         \u001b[39m'\u001b[39m\u001b[39miprint\u001b[39m\u001b[39m'\u001b[39m: iprint,\n\u001b[0;32m    188\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxcor\u001b[39m\u001b[39m'\u001b[39m: m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m'\u001b[39m: callback,\n\u001b[0;32m    195\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxls\u001b[39m\u001b[39m'\u001b[39m: maxls}\n\u001b[1;32m--> 197\u001b[0m res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args\u001b[39m=\u001b[39margs, jac\u001b[39m=\u001b[39mjac, bounds\u001b[39m=\u001b[39mbounds,\n\u001b[0;32m    198\u001b[0m                        \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n\u001b[0;32m    199\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mjac\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    200\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    201\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mfuncalls\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnfev\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    202\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    203\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mwarnflag\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m    204\u001b[0m f \u001b[39m=\u001b[39m res[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:360\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    354\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    361\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[1;32m--> 286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, f0\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                            \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\base\\model.py:531\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(params, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 531\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloglike(params, \u001b[39m*\u001b[39;49margs) \u001b[39m/\u001b[39m nobs\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:939\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[39mif\u001b[39;00m complex_step:\n\u001b[0;32m    937\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minversion_method\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m INVERT_UNIVARIATE \u001b[39m|\u001b[39m SOLVE_LU\n\u001b[1;32m--> 939\u001b[0m loglike \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssm\u001b[39m.\u001b[39mloglike(complex_step\u001b[39m=\u001b[39mcomplex_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    941\u001b[0m \u001b[39m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \u001b[39m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[39m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[39mreturn\u001b[39;00m loglike\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py:983\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[39mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[39m    The joint loglikelihood.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    981\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mconserve_memory\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    982\u001b[0m                   MEMORY_CONSERVE \u001b[39m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[1;32m--> 983\u001b[0m kfilter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    984\u001b[0m loglikelihood_burn \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mloglikelihood_burn\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    985\u001b[0m                                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloglikelihood_burn)\n\u001b[0;32m    986\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (kwargs[\u001b[39m'\u001b[39m\u001b[39mconserve_memory\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p, q, P, Q = (1,1,1,1)\n",
    "endog = top_level_train[\"load\"]\n",
    "exog = top_level_train.drop(columns=[\"load\", \"cos_hour\", \"sin_hour\", \"dow_0\"]).astype(float)\n",
    "top_model = SARIMAX(endog, \n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 1, Q, 24),\n",
    "                            freq = None,\n",
    "                            exog = exog)\n",
    "top_model = top_model.fit()\n",
    "top_model.save('models/SARIMAX/top_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agg in aggregate_train.keys():\n",
    "  endog = aggregate_train[agg][\"load\"]\n",
    "  exog = aggregate_train[agg].drop(columns=\"load\").astype(float)\n",
    "  agg_model = SARIMAX(endog,\n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 1, Q, 24),\n",
    "                            freq = None,\n",
    "                            exog = exog)\n",
    "  agg_model = agg_model.fit(method=\"powell\")\n",
    "  agg_model.save('models/SARIMAX/agg_model_'+ agg +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mid in mid_levels_train.keys():\n",
    "  endog = mid_levels_train[mid][\"load\"]\n",
    "  exog = mid_levels_train[mid].drop(columns=\"load\").astype(float)\n",
    "  mid_model = SARIMAX(endog,\n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 1, Q, 24),\n",
    "                            freq = None,\n",
    "                            exog = exog)\n",
    "  mid_model = mid_model.fit(method=\"powell\")\n",
    "  mid_model.save('models/SARIMAX/mid_model_'+ mid +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bottom in bottom_levels_train.keys():\n",
    "  endog = bottom_levels_train[bottom][\"load\"]\n",
    "  exog = bottom_levels_train[bottom].drop(columns=\"load\").astype(float)\n",
    "  bottom_model = SARIMAX(endog,\n",
    "                            order=(p, 1, q),\n",
    "                            seasonal_order=(P, 1, Q, 24),\n",
    "                            freq = None,\n",
    "                            exog = exog)\n",
    "  bottom_model = bottom_model.fit(method=\"powell\")\n",
    "  bottom_model.save('models/SARIMAX/bottom_model_'+ bottom +'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the hierarchy information\n",
    "meter_ids = pd.unique(joined_data_df[\"meter_id\"])\n",
    "mid_level_ids = pd.unique(joined_data_df[\"mid_level\"])\n",
    "aggregate_level_ids = pd.unique(joined_data_df[\"aggregate\"])\n",
    "n_series = 1 + len(aggregate_level_ids) + len(mid_level_ids) + len(meter_ids)\n",
    "S = np.zeros((n_series, len(meter_ids)))\n",
    "hierarchy_df = pd.read_csv(\"data/hierarchy.csv\")\n",
    "hierarchy_df = hierarchy_df[hierarchy_df[\"meter_id\"].isin(meter_ids)]\n",
    "hierarchy_df[\"indicator\"] = 1\n",
    "aggregate_indicators = hierarchy_df.pivot_table(index=\"aggregate\", columns=\"meter_id\", values=\"indicator\", fill_value=0)\n",
    "mid_level_indicators = hierarchy_df.pivot_table(index=\"mid_level\", columns=\"meter_id\", values=\"indicator\", fill_value=0)\n",
    "\n",
    "# rows / columns\n",
    "row_names = ['Total'] + list(aggregate_level_ids) + list(mid_level_ids) + list(meter_ids)\n",
    "S = pd.DataFrame(S, index=row_names, columns=meter_ids)\n",
    "\n",
    "S.loc['Total', :] = 1\n",
    "for agg in aggregate_level_ids:\n",
    "  S.loc[agg] = aggregate_indicators.loc[agg]\n",
    "for mid in mid_level_ids:\n",
    "  S.loc[mid] = mid_level_indicators.loc[mid]\n",
    "for meter_id in meter_ids:\n",
    "  S.loc[meter_id, meter_id] = 1\n",
    "\n",
    "\n",
    "tags = {}\n",
    "tags['Total'] = np.array(['Total'], dtype=object)\n",
    "tags['Total/Aggregate'] = aggregate_level_ids\n",
    "tags['Total/Aggregate/MidLevel'] = mid_level_ids\n",
    "tags['Total/Aggregate/MidLevel/Meter'] = meter_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = len(top_level_test.index)\n",
    "T = len(top_level_train.index)\n",
    "y_hat = np.zeros((n_series, h))\n",
    "Y_test = np.zeros((n_series, h))\n",
    "Y_train = np.zeros((n_series, T))\n",
    "Y_hat_train = np.zeros((n_series, T))\n",
    "i_series = 0\n",
    "mid_level_dict = hierarchy_df[[\"meter_id\", \"mid_level\"]].set_index(\"meter_id\").to_dict()[\"mid_level\"]\n",
    "\n",
    "top_model = sm.load(\"models/SARIMAX/top_model.pkl\")\n",
    "y_test = top_level_test[\"load\"]\n",
    "X_test = top_level_test.drop(columns=\"load\")\n",
    "y_pred = top_model.predict(X_test)\n",
    "y_train = top_level_train[\"load\"]\n",
    "X_train = top_level_train.drop(columns=\"load\")\n",
    "y_hat_train = top_model.predict(X_train)\n",
    "assert(len(y_train) == T)\n",
    "assert(len(y_pred) == h)\n",
    "y_hat[i_series, :] = y_pred.copy()\n",
    "Y_test[i_series, :] = y_test.to_numpy()\n",
    "Y_train[i_series, :] = y_train.to_numpy()\n",
    "Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "i_series += 1\n",
    "\n",
    "# Aggregate level\n",
    "for agg in aggregate_level_ids:\n",
    "  agg_model = sm.load('models/SARIMAX/agg_model_'+ agg +'.pkl')\n",
    "  y_test = aggregate_test[agg][\"load\"]\n",
    "  X_test = aggregate_test[agg].drop(columns=\"load\")\n",
    "  y_pred = agg_model.predict(X_test)\n",
    "  y_train = aggregate_train[agg][\"load\"]\n",
    "  X_train = aggregate_train[agg].drop(columns=\"load\")\n",
    "  y_hat_train = top_model.predict(X_train)\n",
    "  assert(len(y_train) == T)\n",
    "  assert(len(y_pred) == h)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_test[i_series, :] = y_test.to_numpy()\n",
    "  Y_train[i_series, :] = y_train.to_numpy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1\n",
    "\n",
    "# Mid level\n",
    "for mid in mid_level_ids:\n",
    "  mid_model = sm.load('models/SARIMAX/mid_model_'+ mid +'.pkl')\n",
    "  y_test = mid_levels_test[mid][\"load\"]\n",
    "  X_test = mid_levels_test[mid].drop(columns=\"load\")\n",
    "  y_pred = mid_model.predict(X_test)\n",
    "  y_train = mid_levels_train[mid][\"load\"]\n",
    "  X_train = mid_levels_train[mid].drop(columns=\"load\")\n",
    "  y_hat_train = mid_model.predict(X_train)\n",
    "  assert(len(y_train) == T)\n",
    "  assert(len(y_pred) == h)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_test[i_series, :] = y_test.to_numpy()\n",
    "  Y_train[i_series, :] = y_train.to_numpy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1\n",
    "\n",
    "# Bottom level\n",
    "for bottom in meter_ids:\n",
    "  bottom_model = sm.load('models/SARIMAX/bottom_model_'+ bottom +'.pkl')\n",
    "  y_test = bottom_levels_test[bottom][\"load\"]\n",
    "  X_test = bottom_levels_test[bottom].drop(columns=\"load\")\n",
    "  y_pred = bottom_model.predict(X_test)\n",
    "  y_train = bottom_levels_train[bottom][\"load\"]\n",
    "  X_train = bottom_levels_train[bottom].drop(columns=\"load\")\n",
    "  y_hat_train = bottom_model.predict(X_train)\n",
    "  assert(len(y_train) == T)\n",
    "  assert(len(y_pred) == h)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_test[i_series, :] = y_test.to_numpy()\n",
    "  Y_train[i_series, :] = y_train.to_numpy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_df = pd.DataFrame(y_hat, index=row_names, columns=[dt.datetime(2011,1,1)+i*dt.timedelta(seconds=3600) for i in range(h)])\n",
    "y_hat_df = pd.melt(y_hat_df.reset_index().rename(columns={\"index\": \"unique_id\"}), id_vars=\"unique_id\", var_name=\"ds\", value_name=\"boost\").set_index(\"unique_id\")\n",
    "Y_test = pd.DataFrame(Y_test, index=row_names, columns=[dt.datetime(2011,1,1)+i*dt.timedelta(seconds=3600) for i in range(h)])\n",
    "Y_test = pd.melt(Y_test.reset_index().rename(columns={\"index\": \"unique_id\"}), id_vars=\"unique_id\", var_name=\"ds\", value_name=\"y\").set_index(\"unique_id\")\n",
    "Y_train = pd.DataFrame(Y_train, index=row_names, columns=[dt.datetime(2005,1,1)+i*dt.timedelta(seconds=3600) for i in range(T)])\n",
    "Y_train = pd.melt(Y_train.reset_index().rename(columns={\"index\": \"unique_id\"}), id_vars=\"unique_id\", var_name=\"ds\", value_name=\"y\").set_index(\"unique_id\")\n",
    "Y_hat_train = pd.DataFrame(Y_hat_train, index=row_names, columns=[dt.datetime(2005,1,1)+i*dt.timedelta(seconds=3600) for i in range(T)])\n",
    "Y_hat_train = pd.melt(Y_hat_train.reset_index().rename(columns={\"index\": \"unique_id\"}), id_vars=\"unique_id\", var_name=\"ds\", value_name=\"boost\").set_index(\"unique_id\")\n",
    "Y_train = pd.concat([Y_train, Y_hat_train[\"boost\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcilers = [MinTrace(method=\"ols\"), MinTrace(method=\"wls_struct\"), MinTrace(method=\"wls_var\"), MinTrace(method=\"mint_shrink\"), MinTrace(method=\"mint_cov\"),  BottomUp(), TopDown(method=\"forecast_proportions\")]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "y_hat_rec = hrec.reconcile(y_hat_df, S, tags, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y):\n",
    "    return np.sqrt(mean_squared_error(x,y))\n",
    "evaluator = HierarchicalEvaluation(evaluators=[rmse, mean_absolute_error])\n",
    "evaluator.evaluate(Y_hat_df = y_hat_rec, Y_test_df = Y_test,\n",
    "                   tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.zeros((n_series, h))\n",
    "Y_hat_train = np.zeros((n_series, T))\n",
    "i_series = 0\n",
    "\n",
    "with open('models/prophat/top_model.pkl', 'rb') as model_file:\n",
    "    top_model = pickle.load(model_file)\n",
    "test_df = top_level_test.rename(columns={'timestamp': 'ds', 'load': 'y'})\n",
    "y_pred = top_model.predict(test_df)\n",
    "train_df = top_level_train.rename(columns={'timestamp': 'ds', 'load': 'y'})\n",
    "y_hat_train = top_model.predict(train_df)\n",
    "y_hat[i_series, :] = y_pred.copy()\n",
    "Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "i_series += 1\n",
    "\n",
    "# Aggregate level\n",
    "for agg in aggregate_level_ids:\n",
    "  with open(\"models/prophet/agg_model_\"+agg+\".pkl\", \"rb\") as model_file:\n",
    "    agg_model = pickle.load(model_file)\n",
    "  test_df = aggregate_test[agg].rename(columns={'timestamp': 'ds', 'load': 'y'})\n",
    "  y_pred = agg_model.predict(test_df)\n",
    "  train_df = aggregate_train[agg].rename(columns={\"timestamp\": \"ds\", \"load\": \"y\"})\n",
    "  y_hat_train = agg_model.predict(X_train)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1\n",
    "\n",
    "# Mid level\n",
    "for mid in mid_level_ids:\n",
    "  with open(\"models/prophet/mid_model_\"+mid+\".pkl\", \"rb\") as model_file:\n",
    "    mid_model = pickle.load(model_file)\n",
    "  test_df = mid_levels_test[mid].rename(columns={'timestamp': 'ds', 'load': 'y'})\n",
    "  y_pred = mid_model.predict(test_df)\n",
    "  train_df = mid_levels_train[mid].rename(columns={\"timestamp\": \"ds\", \"load\": \"y\"})\n",
    "  y_hat_train = mid_model.predict(X_train)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1\n",
    "\n",
    "# Bottom level\n",
    "for bottom in meter_ids:\n",
    "  with open(\"models/prophet/bottom_model_\"+bottom+\".pkl\", \"rb\") as model_file:\n",
    "    bottom_model = pickle.load(model_file)\n",
    "  test_df = bottom_levels_test[bottom].rename(columns={'timestamp': 'ds', 'load': 'y'})\n",
    "  y_pred = bottom_model.predict(test_df)\n",
    "  train_df = bottom_levels_train[bottom].rename(columns={\"timestamp\": \"ds\", \"load\": \"y\"})\n",
    "  y_hat_train = bottom_model.predict(X_train)\n",
    "  y_hat[i_series, :] = y_pred.copy()\n",
    "  Y_hat_train[i_series, :] = y_hat_train.copy()\n",
    "  i_series += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcilers = [MinTrace(method=\"ols\"), MinTrace(method=\"wls_struct\"), MinTrace(method=\"wls_var\"), MinTrace(method=\"mint_shrink\"), MinTrace(method=\"mint_cov\"),  BottomUp(), TopDown(method=\"forecast_proportions\")]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "y_hat_rec = hrec.reconcile(y_hat_df, S, tags, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HierarchicalEvaluation(evaluators=[rmse, mean_absolute_error])\n",
    "evaluator.evaluate(Y_hat_df = y_hat_rec, Y_test_df = Y_test,\n",
    "                   tags=tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
